{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "social_dist.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamidrezaGh/Physical_Distancing_Detector/blob/master/social_dist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIQYR0wHg7_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import imutils\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "from scipy.spatial import distance as dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gOo0oijg7_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Application Constant configurations\n",
        "\n",
        "# Minimum probability to filter weak detections\n",
        "MIN_CONFIDENCE = 0.3\n",
        "THERESHOLD = 0.3\n",
        "\n",
        "# Minimum Social Distancing which is safe according to the Healthcare Professionals\n",
        "MIN_DISTANCE = 30  \n",
        "\n",
        "# YOLO directory path\n",
        "MODEL_PATH = \"yolo-coco\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REgpd1MIg7_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_object(frame, network, output_layer, cid):\n",
        "    \n",
        "    \n",
        "    # Get the current frame dmension\n",
        "    (H, W) = frame.shape[:2]\n",
        "    \n",
        "    # initialize variables\n",
        "    confidences = []\n",
        "    bounding_boxes = []\n",
        "    centers = []\n",
        "    results = []\n",
        "\n",
        "    # Getting blob from the current frame\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    network.setInput(blob)\n",
        "    layerOutputs = network.forward(output_layer)\n",
        "\n",
        "    # loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "        # loop over each of the detections\n",
        "        for detection in output:\n",
        "            # extract the confidence and class ID\n",
        "            scores = detection[5:]\n",
        "            confidence = scores[cid]\n",
        "            classID = np.argmax(scores)\n",
        "\n",
        "            # filter only person detection with confidence more that MIN_CONFIDENCE\n",
        "            if classID == cid and confidence > MIN_CONFIDENCE:\n",
        "                # scale the bounding box \n",
        "                box = detection[0:4] * np.array([W, H, W, H])\n",
        "                (center_X, center_Y, width, height) = box.astype(\"int\")\n",
        "\n",
        "                # Get the top left corner of the box\n",
        "                x = int(center_X - (width / 2))\n",
        "                y = int(center_Y - (height / 2))\n",
        "\n",
        "                # append to the lists\n",
        "                confidences.append(float(confidence))\n",
        "                centers.append((center_X, center_Y))\n",
        "                bounding_boxes.append([x, y, int(width), int(height)])\n",
        "                \n",
        "\n",
        "    # apply non-maxima suppression\n",
        "    idxs = cv2.dnn.NMSBoxes(bounding_boxes, confidences, MIN_CONFIDENCE, THERESHOLD)\n",
        "\n",
        "    # check the availability of at least one detection\n",
        "    if len(idxs) > 0:\n",
        "        for i in idxs.flatten():\n",
        "            (x, y) = (bounding_boxes[i][0], bounding_boxes[i][1])\n",
        "            (w, h) = (bounding_boxes[i][2], bounding_boxes[i][3])\n",
        "\n",
        "            # Update the result list and return\n",
        "            r = (confidences[i], (x, y, x + w, y + h), centers[i])\n",
        "            results.append(r)\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l0n091Sg7_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def physical_distancing_detector():\n",
        "    \n",
        "    # Input and Output videos\n",
        "    video = {\n",
        "\n",
        "        'input_video': 'pedestrians.mp4',\n",
        "        'output_video': 'pedestrians_output.mp4',\n",
        "        \"frame_count\": 0,\n",
        "        \"processing_time\": 0\n",
        "    }\n",
        "    \n",
        "    start = time.time()\n",
        "       \n",
        "    # Read the Video File\n",
        "    input_video = cv2.VideoCapture(video[\"input_video\"] if video[\"input_video\"] else 0)\n",
        "    writer = None\n",
        "\n",
        "    # load labels from YOLO model\n",
        "    lables = []\n",
        "    with open(MODEL_PATH + '/coco.names') as file:\n",
        "        lables = [line.strip() for line in file]\n",
        "    \n",
        "    network = cv2.dnn.readNetFromDarknet(MODEL_PATH + '/yolov3.cfg', MODEL_PATH + '/yolov3.weights')\n",
        "  \n",
        "    # Get the output layers that require from YOLO\n",
        "    all_layer = network.getLayerNames()\n",
        "    output_layer = [all_layer[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
        "\n",
        "    \n",
        "    print('start processing the video frame by frame ...')\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        video['frame_count'] = video['frame_count'] + 1\n",
        "\n",
        "        # set of people that violate social distancing role \n",
        "        violateSet = set()\n",
        "        \n",
        "        # Read the next frame\n",
        "        (exist, currentFrame) = input_video.read()\n",
        "\n",
        "        # Check the availability frame, if not (end of the video) the while loop finish\n",
        "        if not exist:\n",
        "            break\n",
        "\n",
        "        # resize the frame and then detect objects (people) in it\n",
        "        currentFrame = imutils.resize(currentFrame, width=500)\n",
        "        results = detect_object(currentFrame, network, output_layer, lables.index(\"person\"))\n",
        "        \n",
        "        if len(results) >= 2:\n",
        "            # Get All centers\n",
        "            centers = np.array([r[2] for r in results])\n",
        "            eucDist = dist.cdist(centers, centers, metric=\"euclidean\")\n",
        "            \n",
        "            for i in range(0, eucDist.shape[0]):\n",
        "                for j in range(i + 1, eucDist.shape[1]):\n",
        "                    if eucDist[i, j] < MIN_DISTANCE:\n",
        "                        violateSet.add(i)\n",
        "                        violateSet.add(j)\n",
        "\n",
        "        # loop over the results to get centroid coordiantes and initialize color\n",
        "        for (i, (prob, box, center)) in enumerate(results):\n",
        "            (center_X, center_Y) = center\n",
        "            (start_X, start_Y, end_X, end_Y) = box\n",
        "            color = (0, 255, 0)\n",
        "            \n",
        "            # If i in the violetSet update the color\n",
        "            if i in violateSet:\n",
        "                color = (0, 0, 255)\n",
        "\n",
        "            cv2.circle(currentFrame, (center_X, center_Y), 5, color, 1)\n",
        "            cv2.rectangle(currentFrame, (start_X, start_Y), (end_X, end_Y), color, 2)\n",
        "            \n",
        "\n",
        "        # Total number of violation cases in the current frame\n",
        "        text = \"Violations Count: \" + format(len(violateSet))\n",
        "        cv2.putText(currentFrame, text, (10, currentFrame.shape[0] - 25),\n",
        "            cv2.FONT_HERSHEY_COMPLEX, 0.9, (36 ,36 ,255), 3)\n",
        "\n",
        "\n",
        "        # Generate output video\n",
        "        if video[\"output_video\"] != \"\" and writer is None:\n",
        "            # initialize our video writer\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "            writer = cv2.VideoWriter(video[\"output_video\"], fourcc, 30,\n",
        "                                     (currentFrame.shape[1], currentFrame.shape[0]), True)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.write(currentFrame)\n",
        "    \n",
        "    end = time.time()\n",
        "\n",
        "    \n",
        "    video['processing_time'] +=  end - start \n",
        "    print('Processing is end and the output file is: ' + video[\"output_video\"])\n",
        "    print()\n",
        "    print('Total number of frames', video['frame_count'])\n",
        "    print('Total processing duration {:.5f} seconds'.format( video['processing_time']))\n",
        "    print('FPS:', round((video['frame_count'] / video['processing_time']), 1))\n",
        "\n",
        "    # Releasing video reader and writer\n",
        "    writer.release()\n",
        "    input_video.release()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDMDRRr4g7_r",
        "colab_type": "code",
        "colab": {},
        "outputId": "9d325162-61a6-4452-b185-195586274590"
      },
      "source": [
        "physical_distancing_detector()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start processing the video frame by frame ...\n",
            "Processing is end and the output file is: pedestrians_output.mp4\n",
            "\n",
            "Total number of frames 532\n",
            "Total processing duration 372.49065 seconds\n",
            "FPS: 1.4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}